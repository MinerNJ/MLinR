install.packages("glmnet")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(ggplot2)
library(caret)
library(glmnet)
set.seed(12345)
N <- 1000
P <- 100
Sigma = diag(sample(1:10, P, P))
X <- mvrnorm(N, runif(P, -10, 10), Sigma)
#Creating a 100x100 matrix for Sigma with a random, repeat sampling of 1 through 10 running through
#the matrix diagonal. Setting X as a multivariate normal with 100 obs, a uniform mean of 0, and Sigma
#as the covariance matrix for the data.
p <- rbinom(P, 1, 0.1)
#vector p from Bernoulli distribution.
beta = p * rnorm(P, 5, sqrt(5)) + (1 - p) * rnorm(P, 0, sqrt(0.1))
epsilon <- rnorm(N, 0, sqrt(10))
y <- X %*% beta + epsilon
#Creating beta to draw from different normal distrubtions based on if p is 1 or 0. Setting epsilon
#as a normal distribution with mean 0 and sd of sqrt(10). Creating y as a function of X times beta
#plus error term epsilon.
dataset1 <- as.data.frame(cbind(y,X))
names(dataset1) <- c("y", rep(paste("X", 1:ncol(X), sep = "")))
train.index <- sample(1:1000, size = 800, replace = FALSE)
test.index <- setdiff(1:1000, train.index)
training.set <- dataset1[train.index,]
test.set <- dataset1[test.index,]
#Creating a dataset from x and Y, then setting the colnames for the dataset. Creating training and
#test sets from a random sampling of the data without overlap.
q2.lm <- lm(y ~ X)
coef(summary(q2.lm))[1:20,]
#Fitting linear model and examining the first 20 coefficients.
#Plotting our data generated beta values against linear model
#estimates. Plotting the data shows a perfect match of the linear
#estimates tothe true values of beta.
plot(beta, coef(q2.lm)[-1], main = "Magnitude of True Beta Parameters vs Linear Estimates",
xlab = "True Values of Beta", ylab = "Linear Model Estimates")
#To test the predictive accuracy of the model, I fit the linear model to my previously created
#training dataset. Then I found the mse of that model to be 8.93. I then fit the training model to
#the previously created test set to create test predictions. I found the mse of the test predictions
#to be 12.21. The training and test mse are reasonably close and reasonably low. Still, a difference
#of over 3 seems to indicate the model might not be the best fit.
training.model <- lm(y~., data = training.set)
training.mse <- mean(((training.set$y - training.model$fitted.values)^2))
training.mse
#setting a linear model to the previously created training data. Then setting and viewing the mean
#squared error of the test set as a manner for assessing model predictive accuracy.
test.preds <- predict(training.model, newdata = test.set)
test.mse <- mean(((test.set$y - test.preds)^2))
test.mse
#Predicting values for the test set using the model fit on the training set. Then finding the mean
#squared error of the test predictions.
#The Elastic Net follows the same pattern as the other plots, with one extreme outlier and much of
#the data centered on (0,0).
plot(sample(beta, size = length(coef(q5.mod.enet2$finalModel, q5.mod.enet2$bestTune$lambda)),
replace = TRUE), coef(q5.mod.enet2$finalModel, q5.mod.enet2$bestTune$lambda),
main = "Magnitude of True Beta Parameters vs Linear Estimates",
xlab = "True Values of Beta", ylab = "Linear Model Estimates")
